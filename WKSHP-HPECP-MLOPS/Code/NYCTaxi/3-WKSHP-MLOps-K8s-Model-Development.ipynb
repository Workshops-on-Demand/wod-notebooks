{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying end-to-end machine learning workflows with HPE Ezmeral ML Ops - Lab 3\n",
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Lab workflow**\n",
    "\n",
    "In this lab: \n",
    "\n",
    "1. You will first create your tenant user's working directory on the central project repository needed in the machine learning (ML) pipeline to store the dataset, trained models and scoring scripts.\n",
    "\n",
    "2. You will then train and test the model on a dataset using a remote tenant-shared training cluster. \n",
    "\n",
    "#### About the Dataset\n",
    "The dataset is based on the 2019 New York City yellow cab trip data (approximately 375,000 trip records from January-June 2019). The dataset has many different properties (aka _\"X - features\"_) like the pickup time and location, the dropoff time and location, the trip distance, the number of passengers, and several other variables. The goal is to predict the taxi ride duration in NY City (the target aka _\"Y - label\"_) based on these features. \n",
    "\n",
    ">**Note: _For this workshop, you will be using a premade subset of this dataset that requires little data preprocessing and preparation (i.e.: data cleaning, data transformation)._**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1- Code versioning integrated with Jupyter Notebook cluster**\n",
    "\n",
    "As you can see in the left hand side, as a data scientist, you have all the files (for example notebooks and code scripts) you need to do your work. These files are all pulled from the GitHub source control repository set up by the Operations team for the data science team.\n",
    "\n",
    "The Jupyter Notebook ML Ops application includes the **Git** plugin that allows data scientists to use Git and do version control of their notebooks and model code scripts directly from within their local Jupyter Notebook.\n",
    "\n",
    "From within the local Jupyter Notebook, data scientists can do the usual _git status_, _git add_, _git commit_, _git push_ to push their notebooks to their GitHub repository branch from the _**Notebook terminal**_ provided, and start versioning their notebooks and codes, and collaborate across projects.\n",
    "If there are other changes that are pushed by other data scientists in the same GitHub repository branch, user can pull up the changes from the terminal using _git pull_ command. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Launcher-terminal.jpg\" height=\"600\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn more about Git\n",
    "\n",
    "Want to learn more about Git, check out the community blog series and the HPE DEV workshop-on-demand:\n",
    "\n",
    "* [Getting started with Git - blog series](https://developer.hpe.com/blog/get-involved-in-the-open-source-community-part-1-getting-started-with-git/)\n",
    "* [Git 101 - Get involved in the open source community - Workshop-on-demand](https://hackshack.hpedev.io/workshops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2- Setup your working directory**\n",
    "\n",
    "The Python code here is used to create the working directory in the central project repository for your tenant userID. The working directory is used to store the key data components needed in the ML pipeline, such as the input dataset, trained ML model(s) and scoring script(s). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userID = \"student{{ STDID }}\"\n",
    "smalldemodataset = True\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "# Shared dataset across all tenant users, use case directory name and scoring file name\n",
    "if smalldemodataset is True:\n",
    "    datasetFile = \"demodata-small.csv\"\n",
    "else:\n",
    "    datasetFile = \"demodata.csv\"\n",
    "    \n",
    "locationTable = \"lookup-ipyheader.csv\"\n",
    "usecaseDirectory = \"NYCTaxi\"\n",
    "scoringFile=\"XGB_Scoring.py\"\n",
    "scoringFileV2=\"XGB_Scoringv2.py\"\n",
    "\n",
    "# Project repo path function - file system mount available to all app containers\n",
    "def ProjectRepo(path):\n",
    "    ProjectRepo = \"/bd-fs-mnt/TenantShare/repo\"\n",
    "    return str(ProjectRepo + '/' + path)\n",
    "\n",
    "print (\"Creating your working directory...\")\n",
    "\n",
    "# Delete existing working directory for userID (that may exists from previous execution of the workshop)\n",
    "studentRepoModel = \"models\" + '/' + usecaseDirectory + '/' + userID\n",
    "studentRepoCode = \"code\" + '/' + usecaseDirectory + '/' + userID\n",
    "pathModel = ProjectRepo(studentRepoModel)\n",
    "pathCode = ProjectRepo(studentRepoCode)\n",
    "\n",
    "if (os.path.exists(ProjectRepo(studentRepoModel))):\n",
    "    #print (\"Repository \" + pathModel + \" exists for user \" + userID + \". Deleting the repo now...\")\n",
    "    shutil.rmtree(pathModel, ignore_errors=True)\n",
    "        \n",
    "if (os.path.exists(ProjectRepo(studentRepoCode))):\n",
    "    #print (\"Repository \" + pathCode + \" exists for user \" + userID + \". Deleting the repo now...\")\n",
    "    shutil.rmtree(pathCode, ignore_errors=True)\n",
    "    \n",
    "# Making sure the input Dataset is loaded and accessible in the shared persistent container storage for your tenant\n",
    "# Check the dataset file exists in /db-fs-mnt/TenantShare/repo/data/NYCTaxi folder:\n",
    "#print (os.listdir(ProjectRepo('data/NYCTaxi')))\n",
    "datasetFilePath = \"data\" + '/' + usecaseDirectory + '/' + datasetFile\n",
    "locationFilePath = \"data\" + '/' + usecaseDirectory + '/' + locationTable\n",
    "pathData = ProjectRepo(\"data\" + '/' + usecaseDirectory)\n",
    "\n",
    "if (not os.path.exists(ProjectRepo(datasetFilePath))):\n",
    "    print (\"Error! Dataset file \" + ProjectRepo(datasetFilePath) + \" does not exist. Please contact the workshop administrator at hpedev.hackshack@hpe.com before continuing.\")\n",
    "if (not os.path.exists(ProjectRepo(locationFilePath))):\n",
    "    print (\"Error! location table file \" + ProjectRepo(locationFilePath) + \" does not exist. Please contact the workshop administrator at hpedev.hackshack@hpe.com before continuing.\")\n",
    "    \n",
    "print (\"Demo dataset is \" + datasetFilePath)\n",
    "\n",
    "# Define the directory structure for the UserID to store trained model and the scoring script\n",
    "# /bd-fs-mnt/TenantShare/repo/models/NYCTaxi/userID; /bd-fs-mnt/TenantShare/repo/code/NYCTaxi/userID\n",
    "\n",
    "if (not os.path.exists(ProjectRepo(studentRepoModel))):\n",
    "    print (\"Creating the Model directory \" + pathModel)\n",
    "    try:\n",
    "        os.makedirs(pathModel)\n",
    "    except OSError:\n",
    "        print (\"Creation of the model directory %s failed\" % pathModel)\n",
    "    else:\n",
    "        print (\"Successfully created the model directory %s\" % pathModel)\n",
    "\n",
    "if (not os.path.exists(ProjectRepo(studentRepoCode))):\n",
    "    print (\"Creating the Code directory \" + pathCode)\n",
    "    try:\n",
    "        os.makedirs(pathCode)\n",
    "    except OSError:\n",
    "        print (\"Creation of the code directory %s failed\" % pathCode)\n",
    "    else:\n",
    "        print (\"Successfully created the code directory %s\" % pathCode)\n",
    "        \n",
    "# Copying scoring script files to your code repository in your working directory\n",
    "# Make sure the scoring script files are available in the /bd-fs-mnt/TenantShare/repo/code/NYCTaxi/userID folder with appropriate permissions (read-execute)\n",
    "##print (\"local directory on your local Jupyter Notebook:\")\n",
    "##print (os.listdir(os.curdir))\n",
    "##print (\"target directory on the shared File System mount for your tenant: \")\n",
    "##print (os.listdir(pathCode))\n",
    "srcFile = scoringFile\n",
    "destFile = pathCode + '/' + scoringFile\n",
    "##srcFileV2 = scoringFileV2\n",
    "##destFileV2 = pathCode + '/' + scoringFileV2\n",
    "\n",
    "if (not os.path.exists(scoringFile)):\n",
    "    print (\"\")\n",
    "    print (\"Error! \" + scoringFile + \" does not exist in your local Jupyter Notebook! Please make sure to copy the file \" + scoringFile + \" to your local Jupyter Notebook, then re-run that code cell to continue the lab.\")\n",
    "else:    \n",
    "    if (os.path.exists(pathCode + '/' + scoringFile)):\n",
    "        print (pathCode + '/' + scoringFile + \" file exists. Setting permissions\")\n",
    "        os.chmod (destFile, 0o777)\n",
    "    else:\n",
    "        #print (\"File\" + pathCode + '/' + scoringFile + \" does not exist.\")\n",
    "        print (\"Copying the scoring script on \" + pathCode + \" and setting file permissions\")\n",
    "        try:\n",
    "            shutil.copy(srcFile, destFile)\n",
    "            os.chmod (destFile, 0o777)\n",
    "        except IOError as e:\n",
    "            print (\"Unable to copy scoring file. %s\" % e )\n",
    "        except:\n",
    "            print (\"Unexpected error:\", sys.exec_info())\n",
    "\n",
    "##if (not os.path.exists(scoringFileV2)):\n",
    "##    print (\"\")\n",
    "##    print (\"Error! \" + scoringFileV2 + \" does not exist in your local Jupyter Notebook! Please make sure to copy the file \" + scoringFileV2 + \" to your local Jupyter Notebook, then re-run that code cell to continue the lab.\")\n",
    "##else:    \n",
    "##    if (os.path.exists(pathCode + '/' + scoringFileV2)):\n",
    "##        print (pathCode + '/' + scoringFileV2 + \" file exists. Setting permissions\")\n",
    "##        os.chmod (destFileV2, 0o777)\n",
    "##    else:\n",
    "##        #print (\"File\" + pathCode + '/' + scoringFileV2 + \" does not exist.\")\n",
    "##        print (\"Copying the scoring script v2 on \" + pathCode + \" and setting file permissions\")\n",
    "##        try:\n",
    "##            shutil.copy(srcFileV2, destFileV2)\n",
    "##            os.chmod (destFileV2, 0o777)\n",
    "##        except IOError as e:\n",
    "##            print (\"Unable to copy scoring file. %s\" % e )\n",
    "##        except:\n",
    "##            print (\"Unexpected error:\", sys.exec_info())\n",
    "            \n",
    "#print (\"target directory  \" + pathCode + \" content :\")\n",
    "#print (os.listdir(pathCode))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3- Test Connection to the remote tenant-shared training cluster**\n",
    "\n",
    "Next, before training your model on remote shared training cluster, you may want to test that the communication with the shared training cluster is indeed functioning properly.\n",
    "\n",
    "> **Note:** _The Jupyter Notebook ML Ops application used in our solution includes **custom magic** functions to handle remotely submitting training code and retrieving results and logs. The Notebook uses these magic functions to make REST API calls to the API server that runs as part of the shared training environment. These calls submit training jobs and get results from within the Notebook session._\n",
    "\n",
    "The Jupyter Notebook ML Ops app includes the following **line magic** functions: \n",
    "*\t%attachments: Returns a list of connected training environments. \n",
    "*\t%logs --url: URL of the training server load balancer used to monitor the status of the training job.\n",
    "\n",
    "The Jupyter Notebook ML Ops app also includes the following **cell magic** function:\n",
    "*\t%%training_cluster_name \n",
    "This would submit training code to the shared training environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Get the list of connected training environments**\n",
    "\n",
    "<b>%attachments</b> is a line magic command that output a table with the name(s) of the training cluster(s) available for us to use. Sometimes, Operations team may have created multiple training clusters for different projects depending on the needs of the model or size of data, e.g. some with GPU nodes, while others with CPUs only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%attachments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Submit a notebook code cell to a remote training cluster**\n",
    "\n",
    "To utilize the training cluster, you will need grab the name of the training cluster you want to use and feed it into another custom cell magic command. \n",
    "\n",
    "With the **%%trainingengineshared** magic command specified at the beginning of the code cell, the Jupyter Notebook will \n",
    "submit the entire content of the cell to the training cluster named _trainingengineshared_. If you comment this magic command, the code will run on your local Jupyter Notebook.\n",
    "\n",
    "The example cell below will execute a print statement on the training cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%trainingengineshared\n",
    "\n",
    "import datetime\n",
    "\n",
    "print('test')\n",
    "print(\"Date time: \", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Retrieve the result of the job**\n",
    "\n",
    "The training cluster will send back a unique log url for the job submitted.   \n",
    "You can use the _History URL_ with the _\"%log --url\"_ custom **line magic** command to track the status of the job in real time. \n",
    "\n",
    "Copy the History URL from the output of the previous cell and paste it into the cell below where it says _\"your_http_url_here\"_, and run the cell code.\n",
    "\n",
    "A status of \"**Finished**\" means the execution of the job submitted to the training cluster has completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##%logs --url your_http_url_here\n",
    "%logs --url your_http_url_here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4- Train the model on the remote shared training cluster**\n",
    "\n",
    "In general, data scientists use their local Jupyter Notebook to **experiment** several learning algorithms with a variety of parameters. They do so to determine the ML model that works best for the business problem they try to address and develop the model that yields to the best prediction result. Then, within their notebooks, they submit their code to large scaled computing training cluster environment to train and test their full ML models, in a reasonable time, typically against a larger training dataset and test dataset. The output of this step is a trained model ready for deployment in production.\n",
    "\n",
    ">**Note:** _This workshop is not intended to teach you about AI/ML model experimentation and development. It is intended to give a use case for data science end-to-end ML workflow with HPE Ezmeral ML Ops. Therefore we will assume that the experimentation step has already been done and that the data science team has shared the best performant ML model in a notebook in the GitHub version control system repository set up by the Operations team for the data science team. The notebook is actually this notebook pulled from GitHub repository by the local Jupyter Notebook cluster. Here you will submit the ML model code to the tenant-shared training cluster environment to train and test your model against the taxi ride training/test dataset._ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About the machine learning model development workflow\n",
    "Gradient boosting supervised machine learning algorithm with Python is used here to build the model that is capable of predicting the duration of taxi trips in New York city. Python libraries such as Numpy, Pandas, Scikit-learn, XGBoost are used to build the model. \n",
    "\n",
    "The machine learning workflow depicted in the diagram below follows the typical supervised machine learning workflow for models development:\n",
    "\n",
    "<img src=\"ML-Workflow.jpg\" height=\"600\" width=\"600\" align=\"right\">\n",
    "\n",
    "- After loading the dataset from the central project repository, the ML algorithm separates data into features (the taxi ride properties) and label (the taxi ride duration). \n",
    "- Then the dataset is divided into two parts, one for training the model and one for testing the model. The typical split is 80/20.\n",
    "- The ML algorithm is then defined and the model is built with the training data set to learn from. \n",
    "- The resulting model is then run on the test data which was not used to train the model.\n",
    "- Next, the model accuracy is evaluated by comparing the test predictions to the test labels. Error metrics such as RMSE are used here to evaluate the predictions accuracy.\n",
    "- The trained model is finally saved to a file in the central project repository. The model is ready for deployment in production to serve predictions.\n",
    "- The next step is then to move your trained model to production to serve your model as a prediction service (Lab Part 4) and make predictions on new data points (Lab Part 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%trainingengineshared\n",
    "\n",
    "userID = \"student{{ STDID }}\"\n",
    "smalldemodataset = True\n",
    "\n",
    "print(\"Importing libraries\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import math\n",
    "import os\n",
    "import datetime\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "usecaseDirectory = \"NYCTaxi\"\n",
    "\n",
    "if smalldemodataset is True:\n",
    "    datasetFile = \"demodata-small.csv\"\n",
    "else:\n",
    "    datasetFile = \"demodata.csv\"\n",
    "    \n",
    "locationTable = \"lookup-ipyheader.csv\"\n",
    "datasetFilePath = \"data\" + '/' + usecaseDirectory + '/' + datasetFile\n",
    "locationFilePath = \"data\" + '/' + usecaseDirectory + '/' + locationTable\n",
    "studentRepoModel = \"models\" + '/' + usecaseDirectory + '/' + userID\n",
    "\n",
    "# Start time \n",
    "print(\"Start time for \" + userID + \": \", datetime.datetime.now())\n",
    "\n",
    "# Project repo path function\n",
    "def ProjectRepo(path):\n",
    "    ProjectRepo = \"/bd-fs-mnt/TenantShare/repo\"\n",
    "    return str(ProjectRepo + '/' + path)\n",
    "\n",
    "\n",
    "print(\"Reading in data\")\n",
    "# Reading in dataset table using pandas\n",
    "dbName = \"pqyellowtaxi\"\n",
    "##df = pd.read_csv(ProjectRepo('data/NYCTaxi/demodata.csv'))\n",
    "df = pd.read_csv(ProjectRepo(datasetFilePath))\n",
    "\n",
    "# Reading in latitude/longitude coordinate lookup table using pandas \n",
    "lookupDbName = \"pqlookup\"\n",
    "##dflook = pd.read_csv(ProjectRepo('data/NYCTaxi/lookup-ipyheader.csv'))\n",
    "dflook = pd.read_csv(ProjectRepo(locationFilePath))\n",
    "print(\"Done reading in data\")\n",
    "\n",
    "\n",
    "# merging dataset and lookup tables on latitudes/coordinates\n",
    "df = pd.merge(df, dflook[[lookupDbName + '.location_i', lookupDbName + '.long', lookupDbName + '.lat']], how='left', left_on=dbName + '.pulocationid', right_on=lookupDbName + '.location_i')\n",
    "df.rename(columns = {(lookupDbName + '.long'):(dbName + '.startstationlongitude')}, inplace = True)\n",
    "df.rename(columns = {(lookupDbName + '.lat'):(dbName + '.startstationlatitude')}, inplace = True)\n",
    "df = pd.merge(df, dflook[[lookupDbName + '.location_i', lookupDbName + '.long', lookupDbName + '.lat']], how='left', left_on=dbName + '.dolocationid', right_on=lookupDbName + '.location_i')\n",
    "df.rename(columns = {(lookupDbName + '.long'):(dbName + '.endstationlongitude')}, inplace = True)\n",
    "df.rename(columns = {(lookupDbName + '.lat'):(dbName + '.endstationlatitude')}, inplace = True)\n",
    "\n",
    "\n",
    "def fullName(colName):\n",
    "    return dbName + '.' + colName\n",
    "\n",
    "# convert string to datetime\n",
    "df[fullName('tpep_pickup_datetime')] = pd.to_datetime(df[fullName('tpep_pickup_datetime')])\n",
    "df[fullName('tpep_dropoff_datetime')] = pd.to_datetime(df[fullName('tpep_dropoff_datetime')])\n",
    "df[fullName('duration')] = (df[fullName(\"tpep_dropoff_datetime\")] - df[fullName(\"tpep_pickup_datetime\")]).dt.total_seconds()\n",
    "\n",
    "# feature engineering\n",
    "# Feature engineering is the process of transforming raw data into inputs for a machine learning algorithm\n",
    "df[fullName(\"weekday\")] = (df[fullName('tpep_pickup_datetime')].dt.dayofweek < 5).astype(float)\n",
    "df[fullName(\"hour\")] = df[fullName('tpep_pickup_datetime')].dt.hour\n",
    "df[fullName(\"work\")] = (df[fullName('weekday')] == 1) & (df[fullName(\"hour\")] >= 8) & (df[fullName(\"hour\")] < 18)\n",
    "df[fullName(\"month\")] = df[fullName('tpep_pickup_datetime')].dt.month\n",
    "# convert month to a categorical feature using one-hot encoding\n",
    "df = pd.get_dummies(df, columns=[fullName(\"month\")])\n",
    "\n",
    "# Filter dataset to rides under 3 hours and under 150 miles to remove outliers\n",
    "df = df[df[fullName('duration')] > 20]\n",
    "df = df[df[fullName('duration')] < 10800]\n",
    "df = df[df[fullName('trip_distance')] > 0]\n",
    "df = df[df[fullName('trip_distance')] < 150]\n",
    "\n",
    "# drop null rows\n",
    "df = df.dropna(how='any',axis=0)\n",
    "\n",
    "# select columns to be used as features\n",
    "cols = [fullName('work'), fullName('startstationlatitude'), fullName('startstationlongitude'), fullName('endstationlatitude'), fullName('endstationlongitude'), fullName('trip_distance'), fullName('weekday'), fullName('hour')]\n",
    "cols.extend([fullName('month_' + str(x)) for x in range(1, 7)])\n",
    "cols.append(fullName('duration'))\n",
    "dataset = df[cols]\n",
    "\n",
    "# separate data into features (the taxi ride properties) and label (duration) using .iloc\n",
    "X = dataset.iloc[:, 0:(len(cols) - 1)].values\n",
    "y = dataset.iloc[:, (len(cols) - 1)].values\n",
    "X = X.copy()\n",
    "y = y.copy()\n",
    "print (X.shape)\n",
    "print (y.shape)\n",
    "del dataset\n",
    "del df\n",
    "\n",
    "print(\"Done cleaning data\")\n",
    "\n",
    "\n",
    "print(\"Training and testing...\")\n",
    "\n",
    "# As we have one dataset, the data is split into a training data set and a test data set. The ideal split is 80:20. \n",
    "# 80% of the data is used to train the model and 20% is used for testing the model.\n",
    "print(\"Load the sklearn module to split the dataset into a training data set and a test data set.\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "print(\"Build the model and fit the model on the training data. This will take a few minutes...\")\n",
    "\n",
    "#Define the ML algorithm (that is the learning algorithm to use). We use here the XGBRegressor class of the xgboost package to build the model\n",
    "xgbr = xgb.XGBRegressor(objective ='reg:squarederror', \n",
    "                        colsample_bytree = 1,\n",
    "                        subsample = 1,\n",
    "                        learning_rate = 0.15,\n",
    "                        booster = \"gbtree\",\n",
    "                        max_depth = 1,\n",
    "                        eta = 0.5,\n",
    "                        eval_metric = \"rmse\",) \n",
    "\n",
    "print(\"num train elements: \" + str(len(X_train)))\n",
    "\n",
    "# then train (fit) the model on training data set comprised of inputs (features) and outputs (label)\n",
    "# we provide our defined ML algorithm with data to learn from\n",
    "print(\"Train start time: \", datetime.datetime.now())\n",
    "xgbr.fit(X_train, y_train)\n",
    "#after training the model, check the model training score. The closer towards 1, the better the fit.\n",
    "score = xgbr.score(X_train, y_train)  \n",
    "print(\"Model training score. The closer towards 1, the better the fit: \", score)\n",
    "\n",
    "print(\"Train end time: \", datetime.datetime.now())\n",
    "print(\"Test the model by making predictions on the test data set where only the features are provided\")\n",
    "y_pred = xgbr.predict(X_test)\n",
    "y_pred = y_pred.clip(min=0)\n",
    "\n",
    "print(\"Evaluating the model accuracy by comparing the test predictions with the test labels. RMSE is used as evaluation metric.\")\n",
    "# evaluating the model accuracy\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print('Root Mean Squared Log Error:', np.sqrt(mean_squared_log_error( y_test, y_pred)))\n",
    "print()\n",
    "print(\"Note that for RMSE the lower that value is, the better the fit\")\n",
    "\n",
    "#after we have trained the model, save it in a pickle file in the tenant user's working directory for later use in production deployment engine\n",
    "#the model will be loaded back from the pickle file using the python scoring script in the deployment engine to make predictions on new data.\n",
    "print(\"Saving model in a pickle file as \" + ProjectRepo(studentRepoModel) + '/' + \"XGB.pickle.dat\")\n",
    "pickle.dump(xgbr, open( ProjectRepo(studentRepoModel) + '/' + \"XGB.pickle.dat\", \"wb\"))\n",
    "\n",
    "# Finish time\n",
    "print(\"End time: \", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5- Monitor the training job**\n",
    "\n",
    "#### Copy the unique log _History URL_ above and paste it into the cell below to monitor your training job (run the cell code below regularly until the job is marked as \"**finished**\")\n",
    "\n",
    ">**Note:** Depending on the number of concurrent jobs submitted to the training cluster environment (i.e: multiple participants run the workshop concurrently), the model training may take several minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##%logs --url your_http_url_here\n",
    "%logs --url your_http_url_here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6- Model registry and deployment**\n",
    "\n",
    "##### After the model development and training, you are now ready to deploy your trained model as prediction service and start serving queries.\n",
    "\n",
    "<font color=\"red\">**Now, let's go back to your JupyterHub account session to continue the hands-on from Lab 4 for model registry and model deployment:**</font>\n",
    "<font color=\"blue\">**4-WKSHP-MLOps-K8s-Register-Model-Deployment.ipynb**.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**======================================================================================**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
