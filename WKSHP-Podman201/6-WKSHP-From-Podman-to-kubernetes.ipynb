{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Podmanlogo](Pictures/podman-logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Podman to Kubernetes and viceversa\n",
    "\n",
    "When working with kubernetes what most users want is a way of easily deploying what they have already tested in Podman. This can be done easily as Podman helps generating a yaml file that can be used afterwards in kubernetes. We demonstrated this on the Podman 101 workshop, but we are going to do it again with our Patient Portal application. Then we'll deploy it to an actual kubernetes cluster (in our case we chose OpenShift distribution).\n",
    "\n",
    "Log into the system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%login {{ hostvars[inventory_hostname]['IP-WKSHP-Podman201'] }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets begin by creating all the containers of our application in Podman. As always, you need to start by creating the networks and the volume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "podman network create database\n",
    "podman network create payment\n",
    "podman volume create patient-portal-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can spin up all of our three containers. We added a sleep in the command list to give enough time for the database to initialize before spining up the frontend container, otherwise the deployment of the app would fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "podman run -d --rm --name database --network database -v patient-portal-data:/pgdata quay.io/skupper/patient-portal-database\n",
    "sleep 10\n",
    "podman run -d --rm --name payment-processor --network payment quay.io/skupper/patient-portal-payment-processor\n",
    "podman run -d --rm --name frontend --network payment,database -p 8080:8080 \\\n",
    "-e DATABASE_SERVICE_HOST=\"database\" \\\n",
    "-e DATABASE_SERVICE_PORT=\"5432\" \\\n",
    "-e PAYMENT_PROCESSOR_SERVICE_HOST=\"payment-processor\" \\\n",
    "-e PAYMENT_PROCESSOR_SERVICE_PORT=\"8080\" \\\n",
    "quay.io/skupper/patient-portal-frontend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check everything is working as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "podman ps -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "podman logs frontend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run an http request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "curl -s localhost:8080"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks fine, it's time to create a yaml file to be used in kubernetes out of our running containers. For that we can simply use the \"podman generate kube\" command. Run it for the database container and review the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "podman generate kube --replicas 1 --type deployment database > database.yml\n",
    "cat database.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the \"--type deployment\" option to specify we want to use a Deployment, otherwise it would have just defined a pod. Moreover you can set the number of replicas of your pod with the \"--replicas\" option.\n",
    "\n",
    "> **Note**: you can also use the \"--service\" option and the command will generate the definition for the service in your yaml file as well as the deployment. We're not using it because it generates a service of the type NodePort which is not the default in kubernetes, we want to use ClusterIP services in our kubernetes cluster so we will create the services manually afterwards.\n",
    ">\n",
    "> For now the only thing you need to know is that NodePort services are used for accessing to the application from outside of the cluster through to the kubernetes node. On the other hand ClusterIP is used for intra cluster communication. We will manually create the services for payment-processor and database as we want them to be internal to the cluster while we will use the NodePort service generated by Podman for the frontend as we want it to be exposed outside of the cluster. If you want to learn more about the different types of services take a look at [the official kubernetes documentation](https://kubernetes.io/docs/concepts/services-networking/service/).\n",
    "\n",
    "As we are pointing the command \"podman generate kube\" command to a single container it will consider that we want to create a pod with a single container in our kubernetes cluster. We could also point to a Podman pod and it would generate a deployment with multiple containers in a pod if that is what we had in the Podman pod.\n",
    "\n",
    "Continue with the payment-processor container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "podman generate kube --replicas 3 --type deployment payment-processor > payment-processor.yml\n",
    "cat payment-processor.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how we specified we want three replicas of this service and, in the yaml file, it's also defined under the \"spec.replicas\" part of the Deployment definition.\n",
    "\n",
    "And, last, for the frontend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "podman generate kube --replicas 1 --service --type deployment frontend > frontend.yml\n",
    "cat frontend.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the frontend we have used the \"--service\" option because we can use a Service of type NodePort as it will be exposed outside of the cluster. Now, lets deploy all these workloads to our kubernetes cluster. Log into the OpenShift cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "oc login -u student{{ STDID}} -p {{ PASSSTU }} --insecure-skip-tls-verify https://{{ OCENDPOINT }}:6443"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new project for our application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "oc new-project patient-portal-student{{ STDID}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by deploying the payment processor container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "oc apply -f payment-processor.yml -n patient-portal-student{{ STDID}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check it's been succesfully deployed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "oc get deployment,pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see all of our resources have been deployed in a super easy way.\n",
    "\n",
    "We're going to deploy our database now as it has to be initialized before the frontend for our application to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "oc apply -f database.yml -n patient-portal-student{{ STDID}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the status of our newly created database workload, use the \"-l\" to filter for the label \"app\" with a value of \"database-pod\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "oc get pods,deployment -l app=database-pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pod status is \"Pending\", lets find why. Take a look at the events in our OpenShift project by using the \"oc get events\" conmmand. Filter it to the last events only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "oc get events | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see in the last event (the one on top) that our pod failed scheduling because it cannot find a persistent volume for our database in any node of the cluster. This is normal because we don't have any storage configured in our OpenShift cluster.\n",
    "\n",
    "To solve this issue you'd need to create a Persistent Volume in kubernetes, as we don't have a storage provider we will just create our pod with no persistent storage. Let's modify our database.yml file, the following command deletes all the lines at the end of the file which are the ones describing the volume and volume mount:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "sed -i '33,$ d' database.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to force the replacement of the resources we created before with the new definitions, delete the previous deployment and apply the file again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "oc delete deployment database-pod-deployment\n",
    "oc apply -f database.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again check the status of our database workload:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "oc get pods,deployment -l app=database-pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you'll see the status of the container as \"Running\", but if you run the previous command again you'll at some point see its status as \"CrashLoopBackOff\". This means the configuration is correct from the kubernetes perspective but there is something wrong in the container or container image itself.\n",
    "\n",
    "Take a look at the last events in case we can find something:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "oc get events | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks fine, it pulled the container image and scheduled it to be deployed. Use the \"oc logs\" command to find what's going on inside the container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "export PODNAME=$(oc get pod -l app=database-pod -o jsonpath=\"{.items[0].metadata.name}\")\n",
    "oc logs $PODNAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the logs we found there is a permissions problem in the container. The postgresql process is trying to initialize the database in a directory that is reserved for priviledged users only, we can solve it by creating the database in a different directory. We looked at the documentation of this container image and it seems that this can be easily done by using a environment variable during the deployment of the container.\n",
    "\n",
    "I want to remark here that OpenShift is a platform build with a huge focus on security and this is a good example of it. The user inside our container is not priviledged so it cannot access to priviledged directories. OpenShift counts with different ways of controling security, one of the most widely used is SCC. We'll not cover this in this workshop, but you can take a look at  [the official documentation](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html/security_and_compliance/container-security-1#security-deployment-sccs_security-platform).\n",
    "\n",
    "Going back to our database container, we now know we need to set the environment variable \"PGDATA\" with a value that points to a user created directory, we'll use \"pgdata/patient-portal\". We can achieve this by adding the environment variable to the database.yml file, but in our case we're going to show how to use the \"oc set\" command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "oc set env deployment/database-pod-deployment PGDATA=/pgdata/patient-portal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That command will modify the yaml file stored by kubernetes and add the environment variable and its value to it. This will automatically trigger a recreation of the container as kubernetes consider this addition important enough to recreate it. You can confirm this by looking at the events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "oc get events | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the left column you see how long ago the events were triggered, you should see the pod was scheduled when you ran the \"oc set\" command. In case you want to force a redeployment of your application, just to be 100% sure it's redeployed, you can use the \"oc rollout restart\" command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "oc rollout restart deployment/database-pod-deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now everything should be working as expected, lets review the logs again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "export PODNAME=$(oc get pod -l app=database-pod -o jsonpath=\"{.items[0].metadata.name}\")\n",
    "oc logs $PODNAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is working fine, we have troubleshooted the deployment of our database!\n",
    "\n",
    "> **Note**: as you can see it took some work to deploy the database container, we could have just configured the Podman container so it doesn't use a volume and uses the environment variable in Podman as well. With that we could generate a yaml file to use with kubernetes without any troubleshooting. We decided to not doing that so we can just show how to troubleshoot this small issues. In general, if you know your application is going to be deployed in kubernetes you may apply some changes to it when working in Podman so it can afterwards be easily deployed in your kubernetes cluster.\n",
    "\n",
    "Last, but not least, we need to deploy our frontend container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "oc apply -f frontend.yml -n patient-portal-student{{ STDID}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check everything has been correctly deployed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "oc get pods,deployment,service -l app=database-pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks fine, lets review the logs of our application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "export PODNAME=$(oc get pod -l app=frontend-pod -o jsonpath=\"{.items[0].metadata.name}\")\n",
    "oc logs $PODNAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe the frontend cannot reach the other microservices. This happens because the database and payment processor microservices are not exposed with a Service object. Lets expose them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "oc expose deployment/database-pod-deployment --port 5432 --name database\n",
    "oc expose deployment/payment-processor-pod-deployment --port 8080 --name payment-processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: our application is hardcoded to look for the \"database\" and \"payment-processor\" DNS names. This is why we have to use the \"--name\" option for the services. Otherwise our frontend pods wouldn't be able to reach the others. Remember the Service name is the DNS name used to reach your microservices.\n",
    "\n",
    "Check both are present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "oc get services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now restart the frontend as it need to get in touch with the other microservices at boot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "oc rollout restart deployment/frontend-pod-deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the logs again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "export PODNAME=$(oc get pod -l app=frontend-pod -o jsonpath=\"{.items[0].metadata.name}\")\n",
    "oc logs $PODNAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our application is now working, but not accessible from outside of the cluster. Expose the frontend service to access to it from outside of kubernetes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "oc expose service/frontend-pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your route has been created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "oc get route"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check it's accessible running a curl command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "export ROUTEADDRESS=$(oc get route frontend-pod -o jsonpath={.spec.host})\n",
    "curl -s $ROUTEADDRESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it works just as expected!\n",
    "\n",
    "> **Note**: in this excercise we have used different yaml files for each microservice. Remember you can use a single file for all of them, you just need to separate each definition with the \"---\" line.\n",
    "\n",
    "We've seen how you can use Podman to generate the files that you'll use later to deploy your workloads in kubernetes. And probably you're thinking that kubernetes yaml files are super cool and you'd like to use them also for Podman workloads. No worries, Podman got your back.\n",
    "\n",
    "Before moving on, stop all of the running containers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "podman rm --all -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you wanted to run your pod in Podman using the kubernetes yaml file you just need to run the following command pointing to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "podman kube play database.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you check the output, it generates a pod with the container within it. This is because in kubernetes we always have a pod and Podman just follows what's in the yaml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "podman ps -a --pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy the payment processor and the frontend using their kubernetes yaml files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "podman kube play payment-processor.yml\n",
    "podman kube play frontend.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing you notice is a warning. Podman's telling us that the amount of replicas for the payment-processor workload has been reduced to 1. This is because Podman is not designed for scalability, it's a single node solution. Therefore it doesn't make sense to have multiple replicas.\n",
    "\n",
    "Review all containers are up and running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "podman ps -a --pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now 6 containers, this is because we have 3 pods and Podman deploys a pod infrastructure management container in every pod.\n",
    "\n",
    "Review frontend logs (we need to use the automatically generated name for the container):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "podman logs frontend-pod-deployment-pod-frontend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also check if the app receives http requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "curl -s localhost:8080"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is working perfectly and using yaml files!\n",
    "\n",
    "There is one last integration between Podman and kubernetes yaml files that I want to mention. Remember Quadlets? They are pretty similar to kubernetes yaml files, your container is defined in them and systemd deploys the containers. But isn't it duplication of work to have both, a Quadlet and a kubernetes yaml file, for the same container? It is! And that is why you can have Quadlets directly pointing to kubernetes yaml files instead of having to define the whole container.\n",
    "\n",
    "Before creating the Quadlets, stop all the Podman workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "podman pod rm --all -f\n",
    "podman rm --all -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check nothing is running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "podman ps -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how these Quadlets would look like. Start with the database one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "cat << EOF > database.kube\n",
    "[Install]\n",
    "WantedBy=default.target\n",
    "\n",
    "[Kube]\n",
    "# Point to the yaml file in the same directory\n",
    "Yaml=database.yml\n",
    "EOF\n",
    "cat database.kube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we just define the target, as always, and then we point to our yaml file in the \"[Kube]\" section. As easy as it is!\n",
    "Rembember we are using a relative path to the yaml file.\n",
    "\n",
    "Do the same for the payment processor container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "cat << EOF > payment-processor.kube\n",
    "[Install]\n",
    "WantedBy=default.target\n",
    "\n",
    "[Unit]\n",
    "Requires=database.service\n",
    "After=database.service\n",
    "\n",
    "[Kube]\n",
    "# Point to the yaml file in the same directory\n",
    "Yaml=payment-processor.yml\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We added the \"[Unit]\" section to guarantee everything is booted in the right order.\n",
    "\n",
    "And for the frontend container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "cat << EOF > frontend.kube\n",
    "[Install]\n",
    "WantedBy=default.target\n",
    "\n",
    "[Unit]\n",
    "Requires=payment-processor.service\n",
    "After=payment-processor.service\n",
    "\n",
    "[Kube]\n",
    "# Point to the yaml file in the same directory\n",
    "Yaml=frontend.yml\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move all the files to \"~/.config/containers/systemd/\". You need to move your yaml file as we used a relative path in the Quadlet definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "mv ~/* ~/.config/containers/systemd/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the files are present in the correct directory you just need to reload the systemctl daemon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "systemctl --user daemon-reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use systemctl to manage your containerized workload as a systemd unit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "systemctl --user start database.service\n",
    "systemctl --user start payment-processor.service\n",
    "systemctl --user start frontend.service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review all of your containers and pods are up and running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "podman ps -a --pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check frontend logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "podman logs frontend-pod-deployment-pod-frontend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And make sure your app is answering http requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "curl -s localhost:8080"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see everything works perfectly!\n",
    "\n",
    "We have seen many ways in which Podman and Kubernetes are interconnected. Hopefully you can leverage all these interconnections to have a better integration and usability in your cloud-native environments.\n",
    "\n",
    "# Podman Desktop and kubernetes\n",
    "\n",
    "Podman Desktop provides a graphical interface to work with Podman, but the most interesting part is the extensions it uses to easily integrate with your kubernetes clusters.\n",
    "\n",
    "Review [this blog post](https://developers.redhat.com/articles/2023/11/06/working-kubernetes-podman-desktop#working_with_remote_kubernetes_clusters) to know more about it!\n",
    "\n",
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "podman rm --all -f\n",
    "podman network prune -f\n",
    "podman volume prune -f\n",
    "podman pod prune -f\n",
    "podman image prune -f\n",
    "rm -rf ~/.config/containers/systemd/*\n",
    "rm database.yml frontend.yml payment-processor.yml\n",
    "oc delete project patient-portal-student{{ STDID}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%logout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## <i class=\"fas fa-2x fa-map-marker-alt\" style=\"color:#631f61;\"></i>&nbsp;&nbsp;Next Steps\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "<h2>Next LAB&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"7-WKSHP-Conclusion.ipynb\" target=\"New\" title=\"Conclusion\"><i class=\"fas fa-chevron-circle-right\" style=\"color:#631f61;\"></i></a></h2>\n",
    "\n",
    "</br>\n",
    " <a href=\"5-WKSHP-Introduction-to-kubernetes.ipynb\" target=\"New\" title=\"Back: Introduction to Kubernetes\"><button type=\"submit\"  class=\"btn btn-lg btn-block\" style=\"background-color:#631f61;color:#fff;position:relative;width:10%; height: 30px;float: left;\"><b>Back</b></button></a>\n",
    " <a href=\"7-WKSHP-Conclusion.ipynb\" target=\"New\" title=\"Conclusion\"><button type=\"submit\"  class=\"btn btn-lg btn-block\" style=\"background-color:#631f61;color:#fff;position:relative;width:10%; height: 30px;float: right;\"><b>Next</b></button></a>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSH",
   "language": "bash",
   "name": "ssh"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "ssh"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
