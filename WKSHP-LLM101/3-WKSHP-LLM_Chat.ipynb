{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Building a Chat Application with LL-Mesh\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, we'll explore how to create a robust chat application using Large Language Models (LLMs) with the help of LL-Mesh's chat services. LL-Mesh provides all the necessary tools to build a powerful chat system by handling:\n",
    "\n",
    "- **Prompt Rendering**\n",
    "- **Model Management**\n",
    "- **Message Processing**\n",
    "- **Memory Management**\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"Pictures/chat.png\" alt=\"LL-Mesh Chat\" width=\"800\">\n",
    "</div>\n",
    "\n",
    "These services are implemented following the **Factory Design Pattern**, which ensures consistency, scalability, and ease of maintenance. Configuration settings and details of the general services are defined in abstract base classes, while instance-specific settings and behaviors are documented within each concrete implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "\n",
    "- Understand how LL-Mesh simplifies the creation of chat applications using LLMs.\n",
    "- Learn how to manage models and memories within the chat service.\n",
    "- Implement prompt rendering and message processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Python 3.11.8+ installed on your system.\n",
    "- LL-Mesh library installed. If not, install it using: `pip install 'llmesh[chat]'`\n",
    "- API keys for the LLM services you plan to use (e.g., OpenAI, Azure).\n",
    "\n",
    "Note: These prerequisites have already been met in the lab environment provided. You do not need to install or configure anything manually. For this session, we will be using Llama 3.0 as the LLM model.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"Pictures/setup.png\" alt=\"LL-Mesh Chat\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Rendering\n",
    "\n",
    "As we learned in the previous notebook, Large Language Models (LLMs) rely heavily on **prompts** to generate meaningful responses. A prompt is essentially the input text that the model uses to understand the context and generate an appropriate output. The effectiveness of an LLM is largely determined by how well these prompts are crafted, making **prompt engineering** a critical aspect of working with LLMs.\n",
    "\n",
    "The **Prompt Rendering** service in LL-Mesh is designed to simplify and automate the creation and management of prompts, which are essential for interacting with LLMs effectively. This service leverages the concepts we explored earlier to ensure that the prompts fed to the model are well-structured, contextually appropriate, and optimized for desired outputs.\n",
    "\n",
    "#### Key Features\n",
    "\n",
    "1. **Dynamic Prompt Creation**: \n",
    "   - Allows for generating prompts from string templates and files.\n",
    "   - Supports placeholders for dynamic content, enabling the insertion of variables (e.g., user input or context) into the prompt structure.\n",
    "  \n",
    "2. **Template-Based Rendering**:\n",
    "   - Uses pre-defined templates to maintain consistent prompt formats, ensuring clarity and coherence.\n",
    "   - Templates can be customized based on the specific use case, enhancing the effectiveness of the prompt.\n",
    "\n",
    "3. **File Management**:\n",
    "   - Enables saving customized prompts back to the file system for reuse.\n",
    "   - Supports versioning and management of different prompt styles, allowing teams to experiment and find the most effective formats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm as notebook_tqdm\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=r'Field \"model_name\" in Config has conflict with protected namespace \"model_\".*',\n",
    "    category=UserWarning,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from athon.chat import PromptRender\n",
    "\n",
    "# Example configuration for the Prompt Render\n",
    "PROMPT_CONFIG = {\n",
    "    'type': 'JinjaTemplate',\n",
    "    'environment': 'Data/',\n",
    "    'templates': {\n",
    "        'welcome': 'welcome_template.txt',\n",
    "        'goodbye': 'goodbye_template.txt'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize the Prompt Render with the provided configuration\n",
    "prompt_render = PromptRender.create(PROMPT_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your name\n",
    "user_name = \"John Doe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render a prompt from a string template\n",
    "template_string = \"Hello, {{ name }}! Welcome to our service.\"\n",
    "render_result = prompt_render.render(template_string, name=user_name)\n",
    "\n",
    "if render_result.status == \"success\":\n",
    "    print(f\"RENDERED CONTENT:\\n{render_result.content}\")\n",
    "else:\n",
    "    print(f\"ERROR:\\n{render_result.error_message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a prompt from a file\n",
    "load_result = prompt_render.load('welcome', name=user_name)\n",
    "\n",
    "if load_result.status == \"success\":\n",
    "    print(f\"LOADED CONTENT:\\n{load_result.content}\")\n",
    "else:\n",
    "    print(f\"ERROR:\\n{load_result.error_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Model Handling\n",
    "\n",
    "The **Chat Model** service in LL-Mesh is designed to handle the instantiation and utilization of various Large Language Models (LLMs) based on user-defined configurations. This service is essential for managing different models and ensuring that the appropriate LLM is used for each specific use case.\n",
    "\n",
    "#### Key Features\n",
    "\n",
    "1. **Model Instantiation**:\n",
    "   - Dynamically creates instances of different LLMs (e.g., OpenAI's GPT, Azure GPT) based on configuration parameters.\n",
    "   - Ensures that the selected model is initialized with the correct settings, such as API keys, temperature, maximum tokens, and other hyperparameters.\n",
    "\n",
    "2. **Model Utilization**:\n",
    "   - Provides a standardized interface for interacting with the instantiated LLMs.\n",
    "   - Manages the execution of prompts and the retrieval of responses from the models.\n",
    "   - Supports seamless switching between models, allowing for flexibility and experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Specify the path to the .env file\n",
    "dotenv_path = os.path.join('Data', '.env')\n",
    "\n",
    "# Load environment variables from the .env file located in the Data folder\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "#Â Read environment variables\n",
    "wod_api_key = os.getenv('WOD_LLM101_API_KEY')\n",
    "if not wod_api_key:\n",
    "    raise ValueError(\"API key is not set in environment variables.\")\n",
    "wod_model = os.getenv('WOD_LLM101_MODEL_NAME')\n",
    "if not wod_model:\n",
    "    raise ValueError(\"Model name is not set in environment variables.\")\n",
    "wod_base_url = os.getenv('WOD_LLM101_BASE_URL')\n",
    "if not wod_base_url:\n",
    "    raise ValueError(\"Base URL is not set in environment variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from athon.chat import ChatModel\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "# Example configuration for the Chat Model\n",
    "LLM_CONFIG = {\n",
    "    'type': 'LangChainChatOpenAI',\n",
    "    'api_key': wod_api_key,\n",
    "    'model_name': wod_model,\n",
    "    'base_url': wod_base_url,\n",
    "    'temperature': 0.7,\n",
    "}\n",
    "\n",
    "# Initialize the Chat Model with the provided configuration\n",
    "chat = ChatModel.create(LLM_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompts\n",
    "prompts = [\n",
    "    SystemMessage(content=\"Convert the message to pirate language\"),\n",
    "    HumanMessage(content=\"Today is a sunny day and the sky is blue\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the model with the prompts\n",
    "result = chat.invoke(prompts)\n",
    "\n",
    "# Handle the response\n",
    "if result.status == \"success\":\n",
    "    print(f\"COMPLETION:\\n{result.content}\")\n",
    "else:\n",
    "    print(f\"ERROR:\\n{result.error_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message Management\n",
    "\n",
    "The **Message Management** service in LL-Mesh handles the serialization and deserialization of messages, ensuring smooth communication between different components of the chat system and compatibility across various data formats. This service is critical for maintaining consistency and enabling effective interactions within and outside the LL-Mesh environment.\n",
    "\n",
    "#### Key Features\n",
    "\n",
    "1. **Message Serialization**:\n",
    "   - Converts message objects into a format suitable for storage or transmission (e.g., JSON, XML).\n",
    "   - Facilitates the exchange of data between different components of the chat system, such as models, memory services, and prompt renderers.\n",
    "   - Ensures messages can be sent to external applications or integrated into various web applications, enhancing interoperability.\n",
    "\n",
    "2. **Message Deserialization**:\n",
    "   - Transforms serialized data back into message objects that can be processed by the chat system.\n",
    "   - Maintains compatibility with multiple data formats, allowing the chat system to receive and handle messages from various sources.\n",
    "   - Supports error handling and validation to ensure that deserialized messages are complete and accurate.\n",
    "\n",
    "3. **Compatibility Across Platforms**:\n",
    "   - Enables messages to be exchanged between LL-Mesh and different web applications, ensuring seamless communication.\n",
    "   - Provides support for multiple serialization formats, making it easy to integrate the chat system with external APIs and third-party tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Memory\n",
    "\n",
    "The **Chat Memory** service in LL-Mesh manages the storage and retrieval of conversation history, which is crucial for maintaining context in chat interactions. This service ensures that the chat application can remember past interactions, enabling it to provide coherent and context-aware responses across multiple turns in a conversation.\n",
    "\n",
    "#### Key Features\n",
    "\n",
    "1. **Storage of Conversation History**:\n",
    "   - Records user inputs and system responses, creating a structured history of the chat session.\n",
    "   - Supports different memory types (e.g., short-term, long-term) to manage context based on the application's needs.\n",
    "   - Utilizes efficient data storage techniques to handle large amounts of conversation data without compromising performance.\n",
    "\n",
    "2. **Retrieval of Context**:\n",
    "   - Provides quick access to relevant parts of the conversation history, ensuring that the LLM has the necessary context to generate appropriate responses.\n",
    "   - Supports customizable retrieval strategies, such as retrieving the last few messages or searching for specific keywords in the conversation history.\n",
    "   - Allows developers to define context windows, determining how much history is retained and referenced in subsequent interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Lab [**Lab 4 Multi-Agent Systems**](4-WKSHP-LLM_MultiAgent.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
