{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a5e4bcf-e5b6-40a3-9135-b8d583505348",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Getting Started with Determined, the Open-Source Deep Learning Training Platform - Lab 2\n",
    "## Distributed training with Determined without requiring any model code changes\n",
    "\n",
    "In this part of the lab, you will learn how to create an experiment that trains a single instance of the model with multiple GPUs, a process known as **Distributed Training**. Again, this experiment will feature a single trial with a set of constant hyperparameters.\n",
    "\n",
    "Determined can coordinate multiple GPUs to train a deep learning model more quickly, leveraging multiple GPUs on single machine or over multiple machines. Typically, ML engineers use Distributed Training to train models on larger datasets in order to improve the model performance and accuracy. This typically requires additional compute resources.\n",
    "\n",
    "Determined automatically executes **[data parallelization](https://www.oreilly.com/content/distributed-tensorflow/)** training, where a data set is divided into multiple pieces and distributed across the GPUs, **without requiring any model code changes**. Each GPU has the full model and trains the model on its portion of the data. Determined ensures the coordination of the training across multiple GPUs on a single machine or multiple machines to keep the whole training task in sync.    \n",
    "\n",
    "> <font color=\"green\"> **Note:** Distributed Training performs best with complex models; therefore, the simple Iris model used in this workshop will not demonstrate the full benefits of using Distributed Training.</font>\n",
    "\n",
    ">_Note: To learn more about Distributed Training with Determined, check out the online documentation [here](https://docs.determined.ai/latest/training-distributed/index.html)._ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab41b4a-d8b2-4b09-a064-cec710e91ef6",
   "metadata": {},
   "source": [
    "## 1- Create an experiment to train a single instance of the model with multiple GPUs (distributed training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f696291e-bcba-48a1-9ca6-ec1f88a9846c",
   "metadata": {},
   "source": [
    "Let's run an experiment with the same model definition (same code), but this time leverage Determined's distributed training functionality using the _distributed.yaml_ experiment configuration file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92247a4-4e2e-416f-ba7d-4fe5f568b9c2",
   "metadata": {},
   "source": [
    "#### Let's take a closer look at the experiment configuration file for distributed training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f45091-4fb7-44a1-8529-18280f25cb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat Code/distributed.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02b4423-1fe0-4c20-9925-8007455e8033",
   "metadata": {},
   "source": [
    "As you can see here, this configuration file is very similar to the _const.yaml_ file you used earlier. All you need to do to start a multi-GPU training workload (trial) is to specify the desired number of GPUs you want to use in the experiment configuration file and Determined takes care of the rest. For example:\n",
    "\n",
    "                                      resources:\n",
    "                                          slots_per_trial: 2\n",
    "\n",
    "With this configuration, the trial within the experiment will use **2 GPUs** to train a single model, whether leveraging 2 GPUs on a single machine or 2 GPUs across multiple machines in the Kubernetes cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accc85db-ffbf-4e5b-8dd8-a727c53c2abd",
   "metadata": {},
   "source": [
    "#### Next, submit the experiment with the experiment configuration file _distributed.yaml_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a16aef-8f05-49a2-bdc1-6892b70ecc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first define the DET_MASTER env variable:\n",
    "masterUrl=$(kubectl describe service determined-master-service-stagingdetai -n determinedai | grep gateway/8080 | awk '{print $3}')\n",
    "determined_master=\"http://${masterUrl}\"\n",
    "export DET_MASTER=${determined_master}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574e8b41-60aa-44c3-a69f-445d413849b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# launch experiment to train a single model on muliple GPUs\n",
    "det experiment create Code/distributed.yaml Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5840e24-bc40-4f57-8ba9-0c978b1b78bb",
   "metadata": {},
   "source": [
    "In the lab environment, the Kubernetes worker hosts have one GPU only. The training task (trial) needs 2 GPUs as per the experiment configuration file. Therefore, Determined Master brings up two PODs for the same trial, with each POD assigned one GPU, on two different Kubernetes worker hosts.   \n",
    "\n",
    "#### Using the command below, you will see that Determined Master has launched **two** PODs for the training task in the Kubernetes cluster with name in the form:\n",
    "\n",
    " _exp-\\<experimentID\\>-trial-\\<TriaID\\>-\\<unique-name\\>_\n",
    "\n",
    "> **Note:** Notice the Trial ID is the same for the two PODs, which means your experiment features a single trial with a fixed set of hyperparameters. \n",
    "\n",
    " > <font color=\"blue\"> **Note:** As you are sharing the same Kubernetes resources with other participants, and depending on the number of concurrent experiments running, your training task POD might be in **Pending** state waiting for GPU resources to become available. You might need to wait a few minutes until other experiments complete for your training task POD to become **Running**.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610cf4e2-b408-4e2d-93b0-6658fc9a0e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pods -n determinedai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d543ff43-cd23-4f57-af3b-76fc8af6cd28",
   "metadata": {},
   "source": [
    "#### Run the code cell below to monitor the execution progress of the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39cfafa-83db-4658-8f5b-3c448c6f8962",
   "metadata": {},
   "outputs": [],
   "source": [
    "det experiment list | tail -1\n",
    "# Get the experiment Id, remove spaces\n",
    "myexpId=$(det experiment list | tail -1 | cut -d'|' -f 1 |  tr -d ' ')\n",
    "#det experiment describe ${myexpId} --json | jq .[0].state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9d0800-e1cb-4eba-9ec9-6157b559edc6",
   "metadata": {},
   "source": [
    "## 2- Monitor and visualize your experiment in Determined Web User Interface\n",
    "\n",
    "To monitor the progress of the training task and access information on both training and validation performance, you can simply return to the Determined **WebUI**.\n",
    "\n",
    "##### From the **Dashboard**, after a minute or so, you should see the experiment as an **active** state and the completion percentage. \n",
    "\n",
    "<img src=\"Pictures/WebUI-Exp-distribute-status.png\" height=\"168\" width=\"400\">\n",
    "\n",
    "> <font color=\"blue\"> **Important Note:** If there are multiple concurrent participants to the workshop, your experiment might not run yet because there are more experiments running than the Kubernetes cluster has GPUs. You might need to wait a few minutes until other experiments complete for your experiment to start running. </font>\n",
    "\n",
    "##### Select the most recent experiment.\n",
    "As the experiment runs, the graph is showing the model **validation** accuracy metric (_val_categorical_accuracy_) for the number of batches on which the model has been trained. You can see the graph changing in real time as the experiment runs.\n",
    "\n",
    "From the **Metrics** menu, under **Training Metrics**, select _categorical_accuracy_. This metric indicates the model accuracy on **training** data while the _val_categorical_accuracy_ indicates the model accuracy on validation data. \n",
    "\n",
    "From the **Metrics** menu, select ***All***. As you can see in the graphs (see picture below for an example), the Master plots training metrics (loss and accuracy) every 100 batches of training data by default. The validation metrics (\"validation\" loss and accuracy) are plotted every 1000 batches based on the experiment configuration parameter _min_validation_period_.\n",
    "\n",
    "\n",
    "After the experiment completes, you can see on the experiment detail page that training the model with the hyperparameter settings in `distributed.yaml` yields a validation accuracy between 93% and 97%. \n",
    "\n",
    "Scroll down to see a list of training validation workloads and their metrics for the metric types you previously selected. \n",
    "You might see one or two validation workloads with checkpoints. By default, Determined will checkpoint the most recent and the best model per training task (trial). If the most recent checkpoint is also the best checkpoint for a given trial, only one checkpoint will be saved for that trial as shown in the picture below.\n",
    "\n",
    "<img src=\"Pictures/WebUI-Exp-distribute-graph.png\" height=\"520\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a7c388-54a6-4f8f-b3a1-73b605948c7e",
   "metadata": {},
   "source": [
    "## 3 - List the best model created by the training process\n",
    "By default, Determined will save the most recent and the best checkpoint per training task (trial) according to the validation metrics specified in the Searcher section of the configuration file for the experiment.\n",
    "\n",
    "* _det experiment list-checkpoints [--best] [N best checkpoints to return] \\<experiment_Id\\>_\n",
    "\n",
    ">**Note**: Upon completion of the training task, if the most recent checkpoint is also the best checkpoint for a given trial, only one checkpoint will be saved for that trial by Determined. Otherwise, two checkpoints will be saved. Other checkpoints will be automatically deleted to reclaim space.\n",
    "\n",
    "#### Run the code cell below to display the best checkpoint for your experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59096343-04b4-47a5-aab4-30244e8b1cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list the best Trial checkpoint(s) (training task):\n",
    "det experiment list-checkpoints --best 2 ${myexpId}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f5d7d0-e7ed-4706-92c0-d9bb020a4f45",
   "metadata": {},
   "source": [
    "## 4- Change the default checkpoint collection policy to reclaim storage space in the shared file system storage\n",
    "\n",
    "The ***save_experiment_best***, ***save_trial_best*** and ***save_trial_latest*** parameters of the checkpoint collection policy specify which checkpoints to save. The default policy is set as follows:\n",
    "\n",
    "  * save_experiment_best:0 \n",
    "  * save_trial_best:1\n",
    "  * save_trial_latest:1\n",
    " \n",
    "The default **checkpoint garbage collection policy** dictates Determined to checkpoint the most recent (the latest) validated model and the best model per training task (trial). The “best” checkpoint for a trial is the checkpoint with the model judged best based on the validation metric defined in the Searcher settings of the experiment configuration file.\n",
    "  \n",
    "#### Run the code cell below to reclaim some storage disk space by changing the default checkpoint garbage collection policy as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd39e584-2caf-4fe1-bb8b-a72f1b88888a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the checkpoints data for the distributed training\n",
    "det experiment set gc-policy --yes --save-experiment-best 0 --save-trial-best 0 --save-trial-latest 0 ${myexpId}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658f8a59-040e-489c-b114-96820feaf017",
   "metadata": {},
   "source": [
    "#### Now, let's explore an Hyperparameter Optimization experiment. \n",
    "Click on Lab 3 below to open a notebook to explore an experiment with Hyperparameter Optimization (HPO). \n",
    "* [Lab 3](3-WKSHP-DET-AI-101-Getting-started-HPO.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
