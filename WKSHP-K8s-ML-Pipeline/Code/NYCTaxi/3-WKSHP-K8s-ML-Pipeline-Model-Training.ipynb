{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pipeline with KubeDirector - Lab 3\n",
    "## Build, train and test your model in a remote tenant-shared training cluster "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Lab workflow**\n",
    "\n",
    "In this lab: \n",
    "\n",
    "1. You will first create your tenant user's working directory on the shared persistent container storage needed in the machine learning (ML) pipeline to store the dataset, trained models and scoring scripts.\n",
    "\n",
    "2. You will then build, train and test the model on a dataset using a remote tenant-shared training cluster. \n",
    "\n",
    "#### About the Dataset\n",
    "The dataset is based on the 2019 New York City yellow cab trip data (approximately 375,000 trip records from January-June 2019). The dataset has many different properties (aka _\"X - features\"_) like the pickup time and location, the dropoff time and location, the trip distance, the number of passengers, and several other variables. The goal is to predict the taxi ride duration in NY City (the target aka _\"Y - label\"_) based on these features. For this workshop, you will be using a premade subset of this dataset that requires little preprocessing. \n",
    "\n",
    "#### About the machine learning model algorithm\n",
    "Gradient boosting supervised machine learning algorithm with Python is used here to build the model that is capable of predicting the duration of taxi trips in New York city. Python libraries such as Numpy, Pandas, Scikit-learn, XGBoost are used to build the model. \n",
    "\n",
    "The machine learning workflow depicted in the diagram below follows the typical supervised machine learning workflow for models development:\n",
    "- After loading the dataset, the ML algorithm separates data into features (the taxi ride properties) and label (the taxi ride duration). \n",
    "- Then the dataset is divided into two parts, one for training the model and one for testing the model. The typical split is 80/20.\n",
    "- The ML algorithm is then defined and the model is built with the training data set to learn from. \n",
    "- The resulting model is then run on the test data which was not used to train the model in order to evaluate the learned models on the validation dataset to get a final idea of how the model might perform on unseen data.\n",
    "- Next, the model accuracy is evaluated by comparing the test predictions to the test labels. Error metrics such as RMSE are used to evaluate the predictions accuracy.\n",
    "- The trained model is finally saved to a file in the tenant user's working directory in the shared persistent container storage repository. The model is saved to a file so that it can be used in production to serve predictions.\n",
    "- The next step is then to move your trained model to production to serve your model as a prediction service (Lab Part 4) and make predictions on new data (Lab Part 5).\n",
    "\n",
    "![ML-Workflow](ML-Workflow.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1- Setup your working directory**\n",
    "\n",
    "The Python code here is used to create the working directory for your tenant userID. The working directory is used to store the key data components needed in the ML pipeline, such as the input dataset, trained ML model(s) and scoring script(s). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userID = \"student{{ STDID }}\"\n",
    "smalldemodataset = True\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "# Shared dataset across all tenant users, use case directory name and scoring file name\n",
    "if smalldemodataset is True:\n",
    "    datasetFile = \"demodata-small.csv\"\n",
    "else:\n",
    "    datasetFile = \"demodata.csv\"\n",
    "    \n",
    "locationTable = \"lookup-ipyheader.csv\"\n",
    "usecaseDirectory = \"NYCTaxi\"\n",
    "scoringFile=\"XGB_Scoring.py\"\n",
    "scoringFileV2=\"XGB_Scoringv2.py\"\n",
    "\n",
    "# Project repo path function - file system mount available to all app containers\n",
    "def ProjectRepo(path):\n",
    "    ProjectRepo = \"/bd-fs-mnt/TenantShare/repo\"\n",
    "    return str(ProjectRepo + '/' + path)\n",
    "\n",
    "print (\"Creating your working directory...\")\n",
    "\n",
    "# Delete existing working directory for userID (that may exists from previous execution of the workshop)\n",
    "##studentRepoModel = \"models\" + '/' + usecaseDirectory + '/' + userID\n",
    "##studentRepoCode = \"code\" + '/' + usecaseDirectory + '/' + userID\n",
    "studentRepoModel = \"models\" + '/' + userID\n",
    "studentRepoCode = \"code\" + '/' + userID\n",
    "pathModel = ProjectRepo(studentRepoModel)\n",
    "pathCode = ProjectRepo(studentRepoCode)\n",
    "\n",
    "if (os.path.exists(ProjectRepo(studentRepoModel))):\n",
    "    #print (\"Repository \" + pathModel + \" exists for user \" + userID + \". Deleting the repo now...\")\n",
    "    shutil.rmtree(pathModel, ignore_errors=True)\n",
    "        \n",
    "if (os.path.exists(ProjectRepo(studentRepoCode))):\n",
    "    #print (\"Repository \" + pathCode + \" exists for user \" + userID + \". Deleting the repo now...\")\n",
    "    shutil.rmtree(pathCode, ignore_errors=True)\n",
    "    \n",
    "# Making sure the input Dataset is loaded and accessible in the shared persistent container storage for your tenant\n",
    "# Check the dataset file exists in /db-fs-mnt/TenantShare/repo/data/NYCTaxi folder:\n",
    "#print (os.listdir(ProjectRepo('data/NYCTaxi')))\n",
    "datasetFilePath = \"data\" + '/' + usecaseDirectory + '/' + datasetFile\n",
    "locationFilePath = \"data\" + '/' + usecaseDirectory + '/' + locationTable\n",
    "pathData = ProjectRepo(\"data\" + '/' + usecaseDirectory)\n",
    "\n",
    "if (not os.path.exists(ProjectRepo(datasetFilePath))):\n",
    "    print (\"Error! Dataset file \" + ProjectRepo(datasetFilePath) + \" does not exist. Please contact the workshop administrator at hpedev.hackshack@hpe.com before continuing.\")\n",
    "if (not os.path.exists(ProjectRepo(locationFilePath))):\n",
    "    print (\"Error! location table file \" + ProjectRepo(locationFilePath) + \" does not exist. Please contact the workshop administrator at hpedev.hackshack@hpe.com before continuing.\")\n",
    "    \n",
    "print (\"Demo dataset is \" + datasetFilePath)\n",
    "\n",
    "# Define the directory structure for the UserID to store trained model and the scoring script\n",
    "# /bd-fs-mnt/TenantShare/repo/models/userID; /bd-fs-mnt/TenantShare/repo/code/userID\n",
    "\n",
    "if (not os.path.exists(ProjectRepo(studentRepoModel))):\n",
    "    print (\"Creating the Model directory \" + pathModel)\n",
    "    try:\n",
    "        os.makedirs(pathModel)\n",
    "    except OSError:\n",
    "        print (\"Creation of the model directory %s failed\" % pathModel)\n",
    "    else:\n",
    "        print (\"Successfully created the model directory %s\" % pathModel)\n",
    "\n",
    "if (not os.path.exists(ProjectRepo(studentRepoCode))):\n",
    "    print (\"Creating the Code directory \" + pathCode)\n",
    "    try:\n",
    "        os.makedirs(pathCode)\n",
    "    except OSError:\n",
    "        print (\"Creation of the code directory %s failed\" % pathCode)\n",
    "    else:\n",
    "        print (\"Successfully created the code directory %s\" % pathCode)\n",
    "        \n",
    "# Copying scoring script files to your code repository in your working directory\n",
    "# Make sure the scoring script files are available in the /bd-fs-mnt/TenantShare/repo/code/NYCTaxi/userID folder with appropriate permissions (read-execute)\n",
    "##print (\"local directory on your local Jupyter Notebook:\")\n",
    "##print (os.listdir(os.curdir))\n",
    "##print (\"target directory on the shared File System mount for your tenant: \")\n",
    "##print (os.listdir(pathCode))\n",
    "srcFile = scoringFile\n",
    "destFile = pathCode + '/' + scoringFile\n",
    "srcFileV2 = scoringFileV2\n",
    "destFileV2 = pathCode + '/' + scoringFileV2\n",
    "\n",
    "if (not os.path.exists(scoringFile)):\n",
    "    print (\"\")\n",
    "    print (\"Error! \" + scoringFile + \" does not exist in your local Jupyter Notebook! Please make sure to copy the file \" + scoringFile + \" to your local Jupyter Notebook, then re-run that code cell to continue the lab.\")\n",
    "else:    \n",
    "    if (os.path.exists(pathCode + '/' + scoringFile)):\n",
    "        print (pathCode + '/' + scoringFile + \" file exists. Setting permissions\")\n",
    "        os.chmod (destFile, 0o777)\n",
    "    else:\n",
    "        #print (\"File\" + pathCode + '/' + scoringFile + \" does not exist.\")\n",
    "        print (\"Copying the scoring script on \" + pathCode + \" and setting file permissions\")\n",
    "        try:\n",
    "            shutil.copy(srcFile, destFile)\n",
    "            os.chmod (destFile, 0o777)\n",
    "        except IOError as e:\n",
    "            print (\"Unable to copy scoring file. %s\" % e )\n",
    "        except:\n",
    "            print (\"Unexpected error:\", sys.exec_info())\n",
    "\n",
    "if (not os.path.exists(scoringFileV2)):\n",
    "    print (\"\")\n",
    "    print (\"Error! \" + scoringFileV2 + \" does not exist in your local Jupyter Notebook! Please make sure to copy the file \" + scoringFileV2 + \" to your local Jupyter Notebook, then re-run that code cell to continue the lab.\")\n",
    "else:    \n",
    "    if (os.path.exists(pathCode + '/' + scoringFileV2)):\n",
    "        print (pathCode + '/' + scoringFileV2 + \" file exists. Setting permissions\")\n",
    "        os.chmod (destFileV2, 0o777)\n",
    "    else:\n",
    "        #print (\"File\" + pathCode + '/' + scoringFileV2 + \" does not exist.\")\n",
    "        print (\"Copying the scoring script v2 on \" + pathCode + \" and setting file permissions\")\n",
    "        try:\n",
    "            shutil.copy(srcFileV2, destFileV2)\n",
    "            os.chmod (destFileV2, 0o777)\n",
    "        except IOError as e:\n",
    "            print (\"Unable to copy scoring file. %s\" % e )\n",
    "        except:\n",
    "            print (\"Unexpected error:\", sys.exec_info())\n",
    "            \n",
    "#print (\"target directory  \" + pathCode + \" content :\")\n",
    "#print (os.listdir(pathCode))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2- Test Connection to the tenant-shared Training Cluster**\n",
    "\n",
    "Next, before training your model on remote shared training cluster, you may want to test that the communication with the shared training cluster is indeed functioning properly.\n",
    "\n",
    "> **Note:** _The KubeDirectorApp Jupyter Notebook used in our solution includes **custom magic** functions to handle remotely submitting training code and retrieving results and logs. The Notebook uses these magic functions to make REST API calls to the API server that runs as part of the shared training environment. These calls submit training jobs and get results from within the Notebook session._\n",
    "\n",
    "The Jupyter Notebook kdapp includes the following **line magic** functions: \n",
    "*\t%attachments: Returns a list of connected training environments. \n",
    "*\t%logs --url: URL of the training server load balancer used to monitor the status of the training job.\n",
    "\n",
    "The Jupyter Notebook kdapp also includes the following **cell magic** function:\n",
    "*\t%%training_cluster_name \n",
    "This would submit training code to the shared training environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Get the list of connected training environments**\n",
    "\n",
    "<b>%attachments</b> is a line magic command that output a table with the name(s) of the training cluster(s) available for us to use. Sometimes, a tenant admin may have created multiple training clusters for different projects depending on the needs of the model or size of data, e.g. some with GPU nodes, while others with CPUs only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%attachments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Submit a code cell to a remote training cluster**\n",
    "\n",
    "To utilize the training cluster, you will need grab the name of the training cluster you want to use and feed it into another custom cell magic command. \n",
    "\n",
    "With the **%%trainingengineshared** magic command specified at the beginning of the code cell, the Jupyter Notebook will \n",
    "submit the entire content of the cell to the training cluster named _trainingengineshared_. If you comment this magic command, the code will run on your local Jupyter Notebook.\n",
    "\n",
    "The **%%capture** magic command captures the stdout/stderr of the code cell in a variable named _history_url_.\n",
    "\n",
    "The example cell below will execute a print statement on the training cluster.\n",
    "\n",
    "**IMPORTANT:** When prompted for a password, please enter the password you used to log in to your local Jupyter notebook server. You will be asked to do so the first time you submit a code cell to the remote training engine cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture history_url\n",
    "\n",
    "%%trainingengineshared\n",
    "import datetime\n",
    "\n",
    "print('test')\n",
    "print(\"Date time: \", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the next code cell to verify you enter the correct password. If you see the message _failed to run training job, Please enter correct password_, execute the code cell above and enter the correct password. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"HTTP/1.1 401 Unauthorized\" in history_url.stdout:\n",
    "    print(\"Failed to run training job, Please enter correct password\")\n",
    "else:\n",
    "    temp = history_url.stdout.split(' ')\n",
    "    for item in temp:\n",
    "        if(item.startswith('http')):\n",
    "            historyurl = item.strip()\n",
    "    print(historyurl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Retrieve the result of the job**\n",
    "\n",
    "The training cluster will send back a unique log url for the job submitted.   \n",
    "You can use the _History URL_ with the _\"%log --url\"_ custom **line magic** command to track the status of the job in real time. \n",
    "\n",
    "A status of \"**Finished**\" means the execution of the job submitted to the training cluster has completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##%logs --url history_http_url_here\n",
    "%logs --url $historyurl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3- Build model remotely on a shared training cluster**\n",
    "\n",
    "You are now ready to submit the  code cell below to the tenant-shared training cluster environment to build, train, and test your model. \n",
    "At the end of the execution of the code cell, the model is saved to a file in your tenant user's working directory for later use in production deployment engine.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture history_url\n",
    "\n",
    "%%trainingengineshared\n",
    "\n",
    "userID = \"student{{ STDID }}\"\n",
    "smalldemodataset = True\n",
    "\n",
    "print(\"Importing libraries\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import math\n",
    "import os\n",
    "import datetime\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "usecaseDirectory = \"NYCTaxi\"\n",
    "\n",
    "if smalldemodataset is True:\n",
    "    datasetFile = \"demodata-small.csv\"\n",
    "else:\n",
    "    datasetFile = \"demodata.csv\"\n",
    "    \n",
    "locationTable = \"lookup-ipyheader.csv\"\n",
    "datasetFilePath = \"data\" + '/' + usecaseDirectory + '/' + datasetFile\n",
    "locationFilePath = \"data\" + '/' + usecaseDirectory + '/' + locationTable\n",
    "#studentRepoModel = \"models\" + '/' + usecaseDirectory + '/' + userID\n",
    "studentRepoModel = \"models\" + '/' + userID\n",
    "\n",
    "# Start time \n",
    "print(\"Start time for \" + userID + \": \", datetime.datetime.now())\n",
    "\n",
    "# Project repo path function\n",
    "def ProjectRepo(path):\n",
    "    ProjectRepo = \"/bd-fs-mnt/TenantShare/repo\"\n",
    "    return str(ProjectRepo + '/' + path)\n",
    "\n",
    "\n",
    "print(\"Reading in data\")\n",
    "# Reading in dataset table using pandas\n",
    "dbName = \"pqyellowtaxi\"\n",
    "##df = pd.read_csv(ProjectRepo('data/NYCTaxi/demodata.csv'))\n",
    "df = pd.read_csv(ProjectRepo(datasetFilePath))\n",
    "\n",
    "# Reading in latitude/longitude coordinate lookup table using pandas \n",
    "lookupDbName = \"pqlookup\"\n",
    "##dflook = pd.read_csv(ProjectRepo('data/NYCTaxi/lookup-ipyheader.csv'))\n",
    "dflook = pd.read_csv(ProjectRepo(locationFilePath))\n",
    "print(\"Done reading in data\")\n",
    "\n",
    "\n",
    "# merging dataset and lookup tables on latitudes/coordinates\n",
    "df = pd.merge(df, dflook[[lookupDbName + '.location_i', lookupDbName + '.long', lookupDbName + '.lat']], how='left', left_on=dbName + '.pulocationid', right_on=lookupDbName + '.location_i')\n",
    "df.rename(columns = {(lookupDbName + '.long'):(dbName + '.startstationlongitude')}, inplace = True)\n",
    "df.rename(columns = {(lookupDbName + '.lat'):(dbName + '.startstationlatitude')}, inplace = True)\n",
    "df = pd.merge(df, dflook[[lookupDbName + '.location_i', lookupDbName + '.long', lookupDbName + '.lat']], how='left', left_on=dbName + '.dolocationid', right_on=lookupDbName + '.location_i')\n",
    "df.rename(columns = {(lookupDbName + '.long'):(dbName + '.endstationlongitude')}, inplace = True)\n",
    "df.rename(columns = {(lookupDbName + '.lat'):(dbName + '.endstationlatitude')}, inplace = True)\n",
    "\n",
    "\n",
    "def fullName(colName):\n",
    "    return dbName + '.' + colName\n",
    "\n",
    "# convert string to datetime\n",
    "df[fullName('tpep_pickup_datetime')] = pd.to_datetime(df[fullName('tpep_pickup_datetime')])\n",
    "df[fullName('tpep_dropoff_datetime')] = pd.to_datetime(df[fullName('tpep_dropoff_datetime')])\n",
    "df[fullName('duration')] = (df[fullName(\"tpep_dropoff_datetime\")] - df[fullName(\"tpep_pickup_datetime\")]).dt.total_seconds()\n",
    "\n",
    "# feature engineering\n",
    "# Feature engineering is the process of transforming raw data into inputs for a machine learning algorithm\n",
    "df[fullName(\"weekday\")] = (df[fullName('tpep_pickup_datetime')].dt.dayofweek < 5).astype(float)\n",
    "df[fullName(\"hour\")] = df[fullName('tpep_pickup_datetime')].dt.hour\n",
    "df[fullName(\"work\")] = (df[fullName('weekday')] == 1) & (df[fullName(\"hour\")] >= 8) & (df[fullName(\"hour\")] < 18)\n",
    "df[fullName(\"month\")] = df[fullName('tpep_pickup_datetime')].dt.month\n",
    "# convert month to a categorical feature using one-hot encoding\n",
    "df = pd.get_dummies(df, columns=[fullName(\"month\")])\n",
    "\n",
    "# Filter dataset to rides under 3 hours and under 150 miles to remove outliers\n",
    "df = df[df[fullName('duration')] > 20]\n",
    "df = df[df[fullName('duration')] < 10800]\n",
    "df = df[df[fullName('trip_distance')] > 0]\n",
    "df = df[df[fullName('trip_distance')] < 150]\n",
    "\n",
    "# drop null rows\n",
    "df = df.dropna(how='any',axis=0)\n",
    "\n",
    "# select columns to be used as features\n",
    "cols = [fullName('work'), fullName('startstationlatitude'), fullName('startstationlongitude'), fullName('endstationlatitude'), fullName('endstationlongitude'), fullName('trip_distance'), fullName('weekday'), fullName('hour')]\n",
    "cols.extend([fullName('month_' + str(x)) for x in range(1, 7)])\n",
    "cols.append(fullName('duration'))\n",
    "dataset = df[cols]\n",
    "\n",
    "# separate data into features (the taxi ride properties) and label (duration) using .iloc\n",
    "X = dataset.iloc[:, 0:(len(cols) - 1)].values\n",
    "y = dataset.iloc[:, (len(cols) - 1)].values\n",
    "X = X.copy()\n",
    "y = y.copy()\n",
    "print (X.shape)\n",
    "print (y.shape)\n",
    "del dataset\n",
    "del df\n",
    "\n",
    "print(\"Done cleaning data\")\n",
    "\n",
    "\n",
    "print(\"Training and testing...\")\n",
    "\n",
    "# As we have one dataset, the data is split into a training data set and a test data set. The ideal split is 80:20. \n",
    "# 80% of the data is used to train the model and 20% is used for testing the model.\n",
    "print(\"Load the sklearn module to split the dataset into a training data set and a test data set.\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "print(\"Build the model and fit the model on the training data. This will take a few minutes...\")\n",
    "\n",
    "#Define the ML algorithm (that is the learning algorithm to use). We use here the XGBRegressor class of the xgboost package to build the model\n",
    "#The hyperparameter tree_method is used to enable training of an XGBoost model using the GPU device.\n",
    "xgbr = xgb.XGBRegressor(objective ='reg:squarederror',\n",
    "                        tree_method = 'gpu_hist',\n",
    "                        colsample_bytree = 1,\n",
    "                        subsample = 1,\n",
    "                        learning_rate = 0.15,\n",
    "                        booster = \"gbtree\",\n",
    "                        max_depth = 1,\n",
    "                        eta = 0.5,\n",
    "                        eval_metric = \"rmse\",) \n",
    "\n",
    "print(\"num train elements: \" + str(len(X_train)))\n",
    "\n",
    "# then train (fit) the model on training data set comprised of inputs (features) and outputs (label)\n",
    "# we provide our defined ML algorithm with data to learn from\n",
    "print(\"Train start time: \", datetime.datetime.now())\n",
    "tmp = datetime.datetime.now()\n",
    "xgbr.fit(X_train, y_train)\n",
    "#after training the model, check the model training score. The closer towards 1, the better the fit.\n",
    "score = xgbr.score(X_train, y_train)  \n",
    "print(\"Model training score. The closer towards 1, the better the fit: \", score)\n",
    "\n",
    "print(\"Train end time: \", datetime.datetime.now())\n",
    "gpu_time = datetime.datetime.now() - tmp\n",
    "print (\"GPU Training time: %s seconds\" %(str(gpu_time)))\n",
    "print(\"Test the model by making predictions on the test data set where only the features are provided\")\n",
    "y_pred = xgbr.predict(X_test)\n",
    "y_pred = y_pred.clip(min=0)\n",
    "\n",
    "print(\"Evaluating the model accuracy by comparing the test predictions with the test labels. RMSE is used as evaluation metric.\")\n",
    "# evaluating the model accuracy\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print('Root Mean Squared Log Error:', np.sqrt(mean_squared_log_error( y_test, y_pred)))\n",
    "print()\n",
    "print(\"Note that for RMSE the lower that value is, the better the fit\")\n",
    "\n",
    "#after we have trained the model, save it in a pickle file in the tenant user's working directory for later use in production deployment engine\n",
    "#the model will be loaded back from the pickle file using the python scoring script in the deployment engine to make predictions on new data.\n",
    "print(\"Saving model in a pickle file as \" + ProjectRepo(studentRepoModel) + '/' + \"XGB.pickle.dat\")\n",
    "pickle.dump(xgbr, open( ProjectRepo(studentRepoModel) + '/' + \"XGB.pickle.dat\", \"wb\"))\n",
    "\n",
    "# Finish time\n",
    "print(\"End time: \", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4- Monitor the training job**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"HTTP/1.1 401 Unauthorized\" in history_url.stdout:\n",
    "    print(\"Failed to run training job on remote training engine.\")\n",
    "else:\n",
    "    temp = history_url.stdout.split(' ')\n",
    "    for item in temp:\n",
    "        if(item.startswith('http')):\n",
    "            historyurl = item.strip()\n",
    "    print(historyurl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the cell code below regularly until the job is marked as \"**finished**\"\n",
    "\n",
    ">**Note:** Depending on the number of concurrent jobs submitted to the training cluster environment (i.e: multiple participants run the workshop concurrently), the model training may take several minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##%logs --url history_http_url_here\n",
    "%logs --url $historyurl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5- Model registry and deployment**\n",
    "\n",
    "##### After the model development and training, you are now ready to deploy your trained model as prediction service and start serving queries.\n",
    "\n",
    "<font color=\"red\">**Now, let's go back to your JupyterHub account session to continue the hands-on from Lab 4 for model registry and model deployment:**</font>\n",
    "<font color=\"blue\">**4-WKSHP-K8s-ML-Pipeline-Register-Model-Deployment.ipynb**.</font>\n",
    "\n",
    "You will use again your local Jupyter Notebook later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**======================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6- Retrain the model to improve model accuracy**\n",
    "Run the cell code below to retrain your model. The \"max_depth\" XGBoost parameter is set to a higher value (value=2 instead of 1) to improve the model accuracy. The model will be saved to a new file as XGB.picklev2.dat in your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture history_url\n",
    "\n",
    "%%trainingengineshared\n",
    "\n",
    "userID = \"student{{ STDID }}\"\n",
    "smalldemodataset = True\n",
    "\n",
    "print(\"Importing libraries\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import math\n",
    "import os\n",
    "import datetime\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "usecaseDirectory = \"NYCTaxi\"\n",
    "\n",
    "if smalldemodataset is True:\n",
    "    datasetFile = \"demodata-small.csv\"\n",
    "else:\n",
    "    datasetFile = \"demodata.csv\"\n",
    "    \n",
    "locationTable = \"lookup-ipyheader.csv\"\n",
    "datasetFilePath = \"data\" + '/' + usecaseDirectory + '/' + datasetFile\n",
    "locationFilePath = \"data\" + '/' + usecaseDirectory + '/' + locationTable\n",
    "#studentRepoModel = \"models\" + '/' + usecaseDirectory + '/' + userID\n",
    "studentRepoModel = \"models\" + '/' + userID\n",
    "\n",
    "# Start time \n",
    "print(\"Start time for \" + userID + \": \", datetime.datetime.now())\n",
    "\n",
    "# Project repo path function\n",
    "def ProjectRepo(path):\n",
    "    ProjectRepo = \"/bd-fs-mnt/TenantShare/repo\"\n",
    "    return str(ProjectRepo + '/' + path)\n",
    "\n",
    "\n",
    "print(\"Reading in data\")\n",
    "# Reading in dataset table using pandas\n",
    "dbName = \"pqyellowtaxi\"\n",
    "##df = pd.read_csv(ProjectRepo('data/NYCTaxi/demodata.csv'))\n",
    "df = pd.read_csv(ProjectRepo(datasetFilePath))\n",
    "\n",
    "# Reading in latitude/longitude coordinate lookup table using pandas \n",
    "lookupDbName = \"pqlookup\"\n",
    "##dflook = pd.read_csv(ProjectRepo('data/NYCTaxi/lookup-ipyheader.csv'))\n",
    "dflook = pd.read_csv(ProjectRepo(locationFilePath))\n",
    "print(\"Done reading in data\")\n",
    "\n",
    "\n",
    "# merging dataset and lookup tables on latitudes/coordinates\n",
    "df = pd.merge(df, dflook[[lookupDbName + '.location_i', lookupDbName + '.long', lookupDbName + '.lat']], how='left', left_on=dbName + '.pulocationid', right_on=lookupDbName + '.location_i')\n",
    "df.rename(columns = {(lookupDbName + '.long'):(dbName + '.startstationlongitude')}, inplace = True)\n",
    "df.rename(columns = {(lookupDbName + '.lat'):(dbName + '.startstationlatitude')}, inplace = True)\n",
    "df = pd.merge(df, dflook[[lookupDbName + '.location_i', lookupDbName + '.long', lookupDbName + '.lat']], how='left', left_on=dbName + '.dolocationid', right_on=lookupDbName + '.location_i')\n",
    "df.rename(columns = {(lookupDbName + '.long'):(dbName + '.endstationlongitude')}, inplace = True)\n",
    "df.rename(columns = {(lookupDbName + '.lat'):(dbName + '.endstationlatitude')}, inplace = True)\n",
    "\n",
    "\n",
    "def fullName(colName):\n",
    "    return dbName + '.' + colName\n",
    "\n",
    "# convert string to datetime\n",
    "df[fullName('tpep_pickup_datetime')] = pd.to_datetime(df[fullName('tpep_pickup_datetime')])\n",
    "df[fullName('tpep_dropoff_datetime')] = pd.to_datetime(df[fullName('tpep_dropoff_datetime')])\n",
    "df[fullName('duration')] = (df[fullName(\"tpep_dropoff_datetime\")] - df[fullName(\"tpep_pickup_datetime\")]).dt.total_seconds()\n",
    "\n",
    "# feature engineering\n",
    "# Feature engineering is the process of transforming raw data into inputs for a machine learning algorithm\n",
    "df[fullName(\"weekday\")] = (df[fullName('tpep_pickup_datetime')].dt.dayofweek < 5).astype(float)\n",
    "df[fullName(\"hour\")] = df[fullName('tpep_pickup_datetime')].dt.hour\n",
    "df[fullName(\"work\")] = (df[fullName('weekday')] == 1) & (df[fullName(\"hour\")] >= 8) & (df[fullName(\"hour\")] < 18)\n",
    "df[fullName(\"month\")] = df[fullName('tpep_pickup_datetime')].dt.month\n",
    "# convert month to a categorical feature using one-hot encoding\n",
    "df = pd.get_dummies(df, columns=[fullName(\"month\")])\n",
    "\n",
    "# Filter dataset to rides under 3 hours and under 150 miles to remove outliers\n",
    "df = df[df[fullName('duration')] > 20]\n",
    "df = df[df[fullName('duration')] < 10800]\n",
    "df = df[df[fullName('trip_distance')] > 0]\n",
    "df = df[df[fullName('trip_distance')] < 150]\n",
    "\n",
    "# drop null rows\n",
    "df = df.dropna(how='any',axis=0)\n",
    "\n",
    "# select columns to be used as features\n",
    "cols = [fullName('work'), fullName('startstationlatitude'), fullName('startstationlongitude'), fullName('endstationlatitude'), fullName('endstationlongitude'), fullName('trip_distance'), fullName('weekday'), fullName('hour')]\n",
    "cols.extend([fullName('month_' + str(x)) for x in range(1, 7)])\n",
    "cols.append(fullName('duration'))\n",
    "dataset = df[cols]\n",
    "\n",
    "# separate data into features (the taxi ride properties) and label (duration) using .iloc\n",
    "X = dataset.iloc[:, 0:(len(cols) - 1)].values\n",
    "y = dataset.iloc[:, (len(cols) - 1)].values\n",
    "X = X.copy()\n",
    "y = y.copy()\n",
    "print (X.shape)\n",
    "print (y.shape)\n",
    "del dataset\n",
    "del df\n",
    "\n",
    "print(\"Done cleaning data\")\n",
    "\n",
    "\n",
    "print(\"Training and testing...\")\n",
    "\n",
    "# As we have one dataset, the data is split into a training data set and a test data set. The ideal split is 80:20. \n",
    "# 80% of the data is used to train the model and 20% is used for testing the model.\n",
    "print(\"Split dataset into a training data set and a test data set.\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "print(\"Build the model and fit the model on the training data. This will take a few minutes...\")\n",
    "\n",
    "#Define the ML algorithm (that is the learning algorithm to use). We use here the XGBRegressor class of the xgboost package to build the model\n",
    "#The hyperparameter tree_method is used to enable training of an XGBoost model using the GPU device. \n",
    "xgbr = xgb.XGBRegressor(objective ='reg:squarederror',\n",
    "                        tree_method = 'gpu_hist',\n",
    "                        colsample_bytree = 1,\n",
    "                        subsample = 1,\n",
    "                        learning_rate = 0.15,\n",
    "                        booster = \"gbtree\",\n",
    "                        max_depth = 2,\n",
    "                        eta = 0.5,\n",
    "                        eval_metric = \"rmse\",) \n",
    "\n",
    "print(\"num train elements: \" + str(len(X_train)))\n",
    "\n",
    "# then train (fit) the model on training data set comprised of inputs (features) and outputs (label)\n",
    "print(\"Train start time: \", datetime.datetime.now())\n",
    "tmp = datetime.datetime.now()\n",
    "xgbr.fit(X_train, y_train)\n",
    "#after training the model, check the model training score. The closer towards 1, the better the fit.\n",
    "score = xgbr.score(X_train, y_train)  \n",
    "print(\"Model training score. The closer towards 1, the better the fit: \", score)\n",
    "\n",
    "print(\"Train end time: \", datetime.datetime.now())\n",
    "gpu_time = datetime.datetime.now() - tmp\n",
    "print (\"GPU Training time: %s seconds\" %(str(gpu_time)))\n",
    "print(\"Test the model by making predictions on the test data set where only the features are provided\")\n",
    "y_pred = xgbr.predict(X_test)\n",
    "y_pred = y_pred.clip(min=0)\n",
    "\n",
    "print(\"Evaluating the model accuracy by comparing the test predictions with the test labels\")\n",
    "# evaluating the model accuracy\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print('Root Mean Squared Log Error:', np.sqrt(mean_squared_log_error( y_test, y_pred)))\n",
    "print()\n",
    "print(\"Note that for RMSE the lower that value is, the better the fit\")\n",
    "\n",
    "\n",
    "print(\"Saving model as \" + ProjectRepo(studentRepoModel) + '/' + \"XGB.picklev2.dat\")\n",
    "pickle.dump(xgbr, open( ProjectRepo(studentRepoModel) + '/' + \"XGB.picklev2.dat\", \"wb\"))\n",
    "\n",
    "\n",
    "# Finish time\n",
    "print(\"End time: \", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7- Monitor the training job** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"HTTP/1.1 401 Unauthorized\" in history_url.stdout:\n",
    "    print(\"Failed to run training job on remote training engine.\")\n",
    "else:\n",
    "    temp = history_url.stdout.split(' ')\n",
    "    for item in temp:\n",
    "        if(item.startswith('http')):\n",
    "            historyurl = item.strip()\n",
    "    print(historyurl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run the cell code below regularly until the job is marked as \"**finished**\".\n",
    "\n",
    ">**Note:** Depending on the number of concurrent jobs submitted to the training cluster environment (i.e: multiple participants run the workshop concurrently), the model training may take several minutes to complete.\n",
    "\n",
    ">**Note:** When the job is \"**Finished**\", observe the _\"Model training score\"_ from the log result. The closer towards 1, the better the performance of the model. The _\"Root Mean Squared Error (RMSE)\"_ can also be used as an indicator of model accuracy and performance. The lower RMSE value is, the better the performance of the model.    \n",
    "You can compare the training score and the RMSE values with their values on the previous training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##%logs --url history_http_url_here\n",
    "%logs --url $historyurl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">**Now go back to your JupyterHub account session to continue the hands-on from Lab 6 step \"3- Adjust the model information registry\" to adjust the model registry information:**</font>\n",
    "\n",
    "<font color=\"blue\">**6-WKSHP-K8s-ML-Pipeline-Dynamic-Aspect.ipynb**.</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
